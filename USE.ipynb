{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wVf_osJDQdM"
      },
      "source": [
        "#Installation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNVdEE4tNP4X",
        "outputId": "697d0f5c-a950-4a60-8127-bd7ee5248c1e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import common_code\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxJn_lhDVjR"
      },
      "source": [
        "#Load USE model and the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Xqc5jtzUDWcD",
        "outputId": "2389252d-d292-431c-b825-b8c63f39e128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Claud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Claud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Claud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Claud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load USE model\n",
        "# warning: it might take 5+ mins to download this model \n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "df_train = pd.read_csv(\"data/train.csv\", sep=\",\")\n",
        "df_test = pd.read_csv(\"data/test.csv\", sep=\",\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start cleaning...\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "USE_EXISTING_CLEANED_FILES = False\n",
        "\n",
        "if USE_EXISTING_CLEANED_FILES:\n",
        "    df_train = pd.read_csv(\"data/cleaned_train.csv\")\n",
        "    df_test = pd.read_csv(\"data/cleaned_test.csv\")\n",
        "else:\n",
        "    df_train, df_test = common_code.clean_text_wrapper(df_train, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASX6ZsjUG_3U"
      },
      "source": [
        "## Sentence Encoder ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "baLmHI-BECI5"
      },
      "outputs": [],
      "source": [
        "#func that take column and encode it using USE and return it\n",
        "def sentences_encoder(col):\n",
        "    sentences = col.tolist()\n",
        "    # Create USE vectors for sentences\n",
        "    sentence_vectors = use_model(sentences)\n",
        "    return sentence_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLuDiWzMN_hg",
        "outputId": "87ae86db-00fd-4fb1-dc53-e00e5acea49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the first sentence encoded result:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              "array([ 1.52491881e-02,  2.40026116e-02, -4.81963307e-02,  4.22714651e-02,\n",
              "        6.96127266e-02,  1.31124798e-02,  1.23990076e-02, -6.05101418e-03,\n",
              "       -2.40373928e-02,  6.81885630e-02,  1.65025219e-02, -6.16021566e-02,\n",
              "        8.09205137e-03, -4.74354252e-02,  2.16256715e-02, -9.46175158e-02,\n",
              "        5.27474517e-03, -2.02022884e-02,  3.91790159e-02, -7.39929378e-02,\n",
              "        4.05305214e-02,  2.56662313e-02,  6.91196322e-02,  6.75732456e-03,\n",
              "       -1.03321504e-02,  4.63298485e-02,  5.33153601e-02, -7.00938404e-02,\n",
              "        7.46023422e-03,  3.92535664e-02,  3.54592688e-02,  3.92078534e-02,\n",
              "        2.56102644e-02,  3.53632942e-02, -2.08516866e-02,  1.05187576e-02,\n",
              "        4.96120565e-03, -3.78673477e-03, -3.78143303e-02, -5.01396973e-03,\n",
              "        1.96129121e-02,  4.35654045e-04,  8.30168091e-03, -8.42987597e-02,\n",
              "       -1.20632192e-02, -7.46258870e-02, -7.25122690e-02,  5.99875972e-02,\n",
              "       -2.88609471e-02,  5.33716269e-02,  4.09351587e-02, -9.47334617e-02,\n",
              "       -3.33246775e-02, -1.44422026e-02,  7.58756360e-04, -3.73237431e-02,\n",
              "        6.53397888e-02, -6.75531551e-02,  3.32641490e-02,  1.93300154e-02,\n",
              "        5.86041994e-02,  3.70557792e-02, -7.44402483e-02,  2.41014883e-02,\n",
              "       -2.23381668e-02, -5.87812485e-03,  4.24586423e-02, -7.30012134e-02,\n",
              "       -2.47470364e-02,  3.18012573e-03, -5.09469304e-03,  1.25110615e-02,\n",
              "        5.06146513e-02, -3.65755521e-02, -1.57093965e-02, -3.19615901e-02,\n",
              "       -4.75733317e-02, -1.17879324e-02, -2.63466556e-02,  4.62086201e-02,\n",
              "        4.14291061e-02,  4.47408184e-02,  3.40169342e-03, -1.80434005e-03,\n",
              "       -7.54637346e-02,  1.56043777e-02,  5.50507149e-03, -2.30118744e-02,\n",
              "       -2.06140243e-02, -6.83533726e-03, -4.23942506e-02,  1.92752518e-02,\n",
              "        4.80560139e-02, -1.23142060e-02, -4.48994040e-02,  2.41522156e-02,\n",
              "       -1.72105152e-02,  8.18833858e-02, -5.46013117e-02,  4.58782464e-02,\n",
              "       -2.92086676e-02, -3.46473232e-02, -7.97699541e-02,  4.83623482e-02,\n",
              "       -6.27656206e-02,  2.68341228e-02, -3.52155827e-02,  5.32333180e-02,\n",
              "       -3.66102196e-02, -4.33929227e-02, -9.10889078e-03, -9.61801559e-02,\n",
              "       -4.10741344e-02, -5.32742552e-02, -8.05452093e-02,  8.43857378e-02,\n",
              "        5.08741885e-02,  3.86123471e-02,  5.22836372e-02, -7.22488575e-03,\n",
              "       -3.05990502e-03, -3.72166955e-03,  8.70179478e-03,  3.16081420e-02,\n",
              "        4.79540527e-02, -6.74893484e-02, -5.29802889e-02, -7.38058910e-02,\n",
              "        3.43054421e-02, -1.94610674e-02, -2.21093819e-02, -4.33943160e-02,\n",
              "        2.30919197e-03,  6.19424060e-02, -5.49429655e-02, -5.43161631e-02,\n",
              "        3.18083214e-03, -2.09432915e-02, -7.37714320e-02, -2.71350667e-02,\n",
              "        7.12031052e-02,  5.37858158e-03,  7.27058500e-02, -5.65611310e-02,\n",
              "        1.53064691e-02, -1.87836064e-03, -7.82785192e-02, -1.70385037e-02,\n",
              "        4.87569608e-02, -2.85695642e-02,  5.42298891e-02,  3.07689756e-02,\n",
              "        2.02996079e-02, -4.20435481e-02, -2.08634567e-02,  3.35240029e-02,\n",
              "        5.64083233e-02, -9.09745023e-02, -1.72849819e-02,  4.99264989e-03,\n",
              "       -3.57405357e-02,  1.44544989e-02,  5.81303015e-02,  2.84494087e-03,\n",
              "       -4.88409437e-02,  4.33797799e-02, -5.59377298e-02, -3.67153548e-02,\n",
              "       -2.55233850e-02, -6.48595300e-03, -4.82436121e-02, -9.09116045e-02,\n",
              "        8.27763602e-03, -2.85079554e-02, -4.00895141e-02,  2.47307401e-03,\n",
              "        3.69089060e-02,  3.63991372e-02,  6.05899245e-02, -5.20046353e-02,\n",
              "        3.58123221e-02, -6.55120658e-03, -9.80646815e-03, -4.96688522e-02,\n",
              "       -1.64899491e-02, -5.04910294e-03, -2.63242070e-02, -4.44200635e-02,\n",
              "       -2.14152094e-02,  1.23374257e-02, -1.93770267e-02,  4.34859507e-02,\n",
              "       -3.54410820e-02,  5.05367443e-02, -1.50846336e-02,  2.72588395e-02,\n",
              "        5.07964604e-02,  2.59346422e-02,  6.04493655e-02,  4.56225350e-02,\n",
              "       -2.71120518e-02,  3.71658355e-02,  4.97642905e-03, -1.42983440e-02,\n",
              "        2.28947494e-02, -2.77273152e-02, -2.34501846e-02, -4.38780487e-02,\n",
              "       -4.62367162e-02,  1.59191694e-02, -4.74117287e-02,  3.23081538e-02,\n",
              "       -3.07514183e-02,  3.82039696e-02, -8.48856345e-02,  5.38799837e-02,\n",
              "        3.55359241e-02, -6.99214786e-02, -1.00516798e-02,  2.71191448e-02,\n",
              "       -1.67571679e-02,  3.64886224e-03, -1.74954850e-02,  1.27661377e-02,\n",
              "       -8.74305959e-04,  7.93860555e-02,  2.19485015e-02,  5.93338981e-02,\n",
              "       -4.26831357e-02, -4.19824347e-02,  2.90546175e-02, -2.33604703e-02,\n",
              "        4.19583991e-02,  9.66957510e-02,  2.95632817e-02, -4.08803970e-02,\n",
              "        5.05801141e-02, -5.42211272e-02, -4.77977097e-02,  5.82255386e-02,\n",
              "        3.60849909e-02, -2.71104369e-03,  1.17688367e-04,  2.42291624e-03,\n",
              "        4.00704108e-02,  9.25296247e-02, -3.92273581e-03,  3.27335261e-02,\n",
              "        6.40892461e-02, -7.64182433e-02,  1.60729773e-02, -4.14131815e-03,\n",
              "        3.34381014e-02,  2.77121998e-02,  3.33885774e-02,  6.70855939e-02,\n",
              "        4.36013378e-03, -1.31948637e-02,  4.21620347e-03, -5.28072938e-02,\n",
              "       -3.47135961e-02, -4.61889664e-03, -5.35851419e-02,  8.50512460e-02,\n",
              "        8.95872638e-02,  3.80936675e-02, -6.39048265e-03,  2.55533531e-02,\n",
              "        3.07641858e-05,  3.70791592e-02, -2.79158191e-03,  7.50972629e-02,\n",
              "        4.25519841e-03, -1.46519411e-02, -6.18023612e-02, -2.12544221e-02,\n",
              "        3.43517363e-02,  2.93992124e-02,  9.08557251e-02,  8.80669355e-02,\n",
              "       -3.67055740e-03,  2.22521066e-03,  5.83870895e-02,  1.42892990e-02,\n",
              "        2.79301442e-02,  9.12792236e-02,  4.50025015e-02,  7.05609620e-02,\n",
              "       -4.24062163e-02, -6.20485144e-03,  6.42452948e-03,  8.38720500e-02,\n",
              "       -6.67089373e-02, -2.26130653e-02,  1.73909441e-02, -9.79550462e-03,\n",
              "        2.28802487e-02,  1.96816958e-02,  7.73385242e-02,  5.49046928e-03,\n",
              "        5.86213805e-02, -3.45567130e-02, -2.32194774e-02, -6.54886961e-02,\n",
              "        6.19252101e-02,  1.15249371e-02, -2.07911972e-02,  1.52425459e-02,\n",
              "        1.21057015e-02,  5.90901189e-02, -3.39457802e-02,  3.26918997e-02,\n",
              "        6.45134673e-02, -3.65147591e-02,  5.97238094e-02,  3.20986770e-02,\n",
              "       -3.87023538e-02,  3.42220627e-02,  5.78323677e-02, -6.23947522e-03,\n",
              "       -6.44809306e-02, -5.40114706e-03,  5.77281602e-02,  7.23144971e-03,\n",
              "       -1.72838680e-02,  1.36436457e-02, -5.37160970e-03, -3.39985080e-02,\n",
              "        2.67919786e-02, -1.64656118e-02, -4.42607179e-02, -1.57440230e-02,\n",
              "       -4.25476059e-02,  7.12626576e-02, -8.74147490e-02,  3.01943664e-02,\n",
              "        1.04377046e-03, -5.42323329e-02,  8.18223879e-02,  2.07398832e-02,\n",
              "       -1.06291361e-02,  1.20402565e-02, -5.07011525e-02, -1.39573338e-02,\n",
              "        5.47879301e-02, -4.20160517e-02,  6.51454106e-02, -9.12568718e-03,\n",
              "       -5.25030605e-02, -3.50970514e-02,  3.45426798e-02, -4.39273417e-02,\n",
              "        2.85341237e-02, -4.53875586e-02, -1.08206728e-02,  2.32953904e-03,\n",
              "       -9.38398018e-02, -1.95233971e-02,  2.58380342e-02, -2.59268489e-02,\n",
              "        3.82843055e-02,  4.52828739e-04,  9.66617316e-02,  6.12636134e-02,\n",
              "       -6.14759140e-03, -3.23758125e-02, -9.27638710e-02,  2.42908169e-02,\n",
              "        3.60268429e-02,  6.22023712e-04, -6.73426986e-02,  2.08842903e-02,\n",
              "        3.32443602e-02,  2.56414320e-02,  6.51944578e-02, -1.88207682e-02,\n",
              "       -2.82455646e-02,  5.67325614e-02, -1.07587390e-02, -3.40342708e-02,\n",
              "       -1.31318381e-03,  1.53497085e-02, -7.99429417e-02,  7.94522688e-02,\n",
              "        7.43392408e-02, -8.99443254e-02, -9.66422111e-02,  3.83005962e-02,\n",
              "        4.20283154e-02, -2.56853849e-02,  7.79991820e-02,  1.24342768e-02,\n",
              "        1.45369358e-02,  7.05748349e-02,  1.56499967e-02,  9.49663296e-02,\n",
              "        3.14043723e-02,  4.21159118e-02,  2.04135142e-02, -5.49074747e-02,\n",
              "       -3.49975489e-02, -3.89813296e-02,  1.94393862e-02, -1.81352757e-02,\n",
              "        1.98584236e-02, -2.31932625e-02, -7.13931769e-02, -4.08789888e-02,\n",
              "       -9.01437104e-02,  1.84579343e-02,  1.06008351e-02,  8.15922208e-03,\n",
              "       -1.24816578e-02, -1.37032373e-04,  2.63988394e-02,  1.80023257e-02,\n",
              "       -8.93144682e-03, -4.63456772e-02, -3.26185627e-03,  5.19889221e-02,\n",
              "       -3.42181064e-02,  7.70921111e-02, -1.61393471e-02,  9.88559611e-03,\n",
              "        2.37714257e-02,  8.50196630e-02,  2.59590074e-02,  5.71412332e-02,\n",
              "       -6.37480915e-02,  9.70740896e-03,  2.60491353e-02, -2.09696312e-02,\n",
              "       -3.17893699e-02, -3.46246809e-02, -2.77246605e-03, -7.48605058e-02,\n",
              "        4.40155230e-02,  1.57949254e-02, -5.92166893e-02, -6.75374418e-02,\n",
              "       -5.49232252e-02, -1.35674570e-02, -1.26984101e-02,  1.42861428e-02,\n",
              "        4.54435265e-03, -7.16649089e-03,  4.05181274e-02, -2.77045630e-02,\n",
              "       -4.71032895e-02, -2.66864877e-02, -1.54299363e-02, -8.73727277e-02,\n",
              "        7.33620524e-02, -3.61648016e-02, -7.06199333e-02,  4.65111248e-02,\n",
              "       -2.55132690e-02, -5.87058589e-02, -7.82099888e-02,  3.46596017e-02,\n",
              "       -7.14291353e-03, -3.90358977e-02,  3.44331563e-02, -1.46338781e-02,\n",
              "        8.55321065e-02, -8.83471314e-03,  6.65726662e-02, -2.19579265e-02,\n",
              "        4.74256761e-02,  3.00820600e-02, -5.69970682e-02, -4.14149389e-02,\n",
              "       -1.13912243e-02,  3.90006811e-03,  4.22177725e-02, -5.12515269e-02,\n",
              "       -3.42542678e-02, -4.79300767e-02,  9.06181931e-02,  8.52261484e-02,\n",
              "        7.29917884e-02,  2.93556927e-03,  5.54181784e-02, -5.42906746e-02,\n",
              "        3.32724676e-02,  2.96843294e-02,  5.91921806e-02,  2.45814323e-02,\n",
              "       -4.15319465e-02, -3.27166356e-03, -3.87111343e-02, -5.72056584e-02,\n",
              "        5.65573610e-02, -7.67894415e-03,  7.57353082e-02, -2.09319834e-02,\n",
              "        2.64173709e-02, -2.67592072e-02,  2.00991537e-02,  4.57263701e-02,\n",
              "       -3.90798748e-02, -2.43253689e-02,  2.99983546e-02, -7.27108195e-02,\n",
              "        5.61651252e-02,  5.10894582e-02,  2.74219667e-03, -2.28224285e-02,\n",
              "        4.63750325e-02, -4.62016650e-02,  1.54032614e-02,  5.07257357e-02],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "train_sentence_vectors = sentences_encoder(df_train['cleaned_text'])\n",
        "test_sentence_vectors = sentences_encoder(df_train['cleaned_text'])\n",
        "\n",
        "print(\"the first sentence encoded result:\")\n",
        "train_sentence_vectors[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Y labels to binorminal values ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6KPCZ74Up7P",
        "outputId": "aa1bb6e9-f4d3-4b5e-93a3-52c3a9079e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name maps to numeric label:  {'amusement': 0, 'anger': 1, 'disapproval': 2, 'gratitude': 3, 'love': 4}\n",
            "numeric label maps to name:  {0: 'amusement', 1: 'anger', 2: 'disapproval', 3: 'gratitude', 4: 'love'}\n"
          ]
        }
      ],
      "source": [
        "le = LabelEncoder()\n",
        "df_train[\"upt_label\"] = le.fit_transform(df_train['Label'])\n",
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "le_label_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
        "print(\"name maps to numeric label: \", le_name_mapping)\n",
        "print(\"numeric label maps to name: \", le_label_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-pR6DZZHGsp"
      },
      "source": [
        "# Train and Create the model #\n",
        "## Logistic Regression ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "-ZxCk9kdOIV9",
        "outputId": "be3be9f2-fdad-43dc-eb80-9cd0d174fb12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data shape:\t\t (6750, 512) (6750,)\n",
            "Verification Data shape:\t (750, 512) (750,)\n"
          ]
        }
      ],
      "source": [
        "# split the dataset for training and testing\n",
        "indices = np.arange(df_train.shape[0])\n",
        "X_train, X_veri, y_train, y_veri, indices_train, indices_veri = train_test_split(\n",
        "    pd.DataFrame(train_sentence_vectors), df_train['upt_label'], indices, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Training Data shape:\\t\\t\", X_train.shape, y_train.shape)\n",
        "print(\"Verification Data shape:\\t\", X_veri.shape, y_veri.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hXQW7Bwjora",
        "outputId": "8407b7d8-02fc-4187-af0f-6c653cc7e876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "Logistic Regression accuracy: 0.77\n",
            "Logistic F1 Score micro: 0.7706666666666667\n",
            "Logistic F1 Score macro: 0.7600537543052039\n",
            "Logistic confusion matrix:\n",
            "[[116  11  11   6   5]\n",
            " [  7  66  35   4   4]\n",
            " [ 12  20 123   4   7]\n",
            " [  3   5   7 161   5]\n",
            " [  7   6   9   4 112]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "log_model = LogisticRegression(C=0.85, solver=\"saga\", max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "y_veri_pred = log_model.predict(X_veri)\n",
        "\n",
        "print('\\n------------------------------')\n",
        "print(\"Logistic Regression accuracy: {:.2f}\".format(metrics.accuracy_score(y_veri, y_veri_pred)))\n",
        "print(\"Logistic F1 Score micro:\", metrics.f1_score(y_veri, y_veri_pred, average='micro'))\n",
        "print(\"Logistic F1 Score macro:\", metrics.f1_score(y_veri, y_veri_pred, average='macro'))\n",
        "print(\"Logistic confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_veri, y_veri_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h23zFbllHVhE"
      },
      "source": [
        "## SVM ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "4XFP0FF0zjxY",
        "outputId": "1902bc34-a6f4-4b8e-83c4-16b542407729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVC Train accuracy: 0.81\n",
            "SVC Regression accuracy: 0.76\n",
            "SVC F1 Score micro: 0.7573333333333333\n",
            "SVC F1 Score macro: 0.7491267102955541\n",
            "SVC confusion matrix:\n",
            "[[113  13  10   6   7]\n",
            " [  6  69  35   4   2]\n",
            " [ 13  21 120   6   6]\n",
            " [  6   7   7 155   6]\n",
            " [  7   7   9   4 111]]\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train a Linear SVM classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_train_pred = svm_model.predict(X_train)\n",
        "y_veri_pred = svm_model.predict(X_veri)\n",
        "print(\"SVC Train accuracy: {:.2f}\".format(metrics.accuracy_score(y_train, y_train_pred)))\n",
        "print(\"SVC Regression accuracy: {:.2f}\".format(metrics.accuracy_score(y_veri, y_veri_pred)))\n",
        "print(\"SVC F1 Score micro:\", metrics.f1_score(y_veri, y_veri_pred, average='micro'))\n",
        "print(\"SVC F1 Score macro:\", metrics.f1_score(y_veri, y_veri_pred, average='macro'))\n",
        "print(\"SVC confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_veri, y_veri_pred))\n",
        "print('------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjdCApugHYvl"
      },
      "source": [
        "## RandomForst ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "48Kp0rlz33gA",
        "outputId": "ef117676-470f-464e-e619-6e88b208d82a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Randomforest training accuracy: 1.00\n",
            "Randomforest verification accuracy: 0.73\n",
            "Randomforest F1 Score micro: 0.732\n",
            "Randomforest F1 Score macro: 0.7196979895334621\n",
            "Randomforest confusion matrix:\n",
            "[[110  11  12   8   8]\n",
            " [  4  54  47   8   3]\n",
            " [ 10  16 130   4   6]\n",
            " [  4   5  14 155   3]\n",
            " [  7   4  23   4 100]]\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_veri_pred = rf_model.predict(X_veri)\n",
        "print(\"Randomforest training accuracy: {:.2f}\".format(metrics.accuracy_score(y_train, y_train_pred)))\n",
        "print(\"Randomforest verification accuracy: {:.2f}\".format(metrics.accuracy_score(y_veri, y_veri_pred)))\n",
        "print(\"Randomforest F1 Score micro:\", metrics.f1_score(y_veri, y_veri_pred, average='micro'))\n",
        "print(\"Randomforest F1 Score macro:\", metrics.f1_score(y_veri, y_veri_pred, average='macro'))\n",
        "print(\"Randomforest confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_veri, y_veri_pred))\n",
        "print('------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKFn_0W9Aws_"
      },
      "source": [
        "#What is USE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWCd5QAQAzdl"
      },
      "source": [
        "(USE) The Universal Sentence Encoder  is model that encodes text into high-dimensional vectors that can be used for a variety of natural language processing (NLP) tasks. It was developed by Google AI and is available as a pre-trained model on the TensorFlow Hub.\n",
        "\n",
        "\n",
        "The output of the model is a 512-dimensional vector that represents the meaning of the input text.\n",
        "\n",
        "there two  main verision for USE one is Transformers Based and the other DAN based\n",
        "we are using the DAN based version ,\n",
        " The model is a deep learning model that uses a technique called deep averaging network (DAN).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The USE model can be used for a variety of NLP tasks, including:\n",
        "\n",
        "Text classification: Categorizing text into different classes, such as news, social media, or product reviews.\n",
        "\n",
        "Semantic similarity: Measuring the similarity between two pieces of text.\n",
        "\n",
        "Clustering: Grouping similar pieces of text together.\n",
        "\n",
        "Question answering: Answering questions about text.\n",
        "\n",
        "Machine translation: Translating text from one language to another.\n",
        "\n",
        "\n",
        "Unlike traditional word embeddings like Word2Vec or GloVe that generate vectors for individual words, the Universal Sentence Encoder focuses on entire sentences or short text snippets.\n",
        "\n",
        "\n",
        " This makes it particularly useful for scenarios where understanding the meaning of a full sentence in context is important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start cleaning...\n",
            "done!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Text</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i love this i am big fun of cross over and Lor...</td>\n",
              "      <td>love big fun cross lore skin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FUCK LES BRUNS</td>\n",
              "      <td>fuck les brun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Anyone who can pass the test proceeds. The sta...</td>\n",
              "      <td>anyone pass test proceed . standard exist prot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>yeah you're right. i just glanced over and tho...</td>\n",
              "      <td>ye relief , happy right . glance think swish l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I get abused on the daily fam</td>\n",
              "      <td>get abuse daily fam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>2495</td>\n",
              "      <td>[NAME] pnr play has not been good, they are de...</td>\n",
              "      <td>pnr play not good , defend jok let get rim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>You k how it's bad when a furry is roasting you</td>\n",
              "      <td>k bad furry roast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>Don't you know that kids need constant supervi...</td>\n",
              "      <td>not know kid need constant supervision turn ? ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>DON'T DO THE TEENAGE MUTANT NINJA TURTLES LIKE...</td>\n",
              "      <td>not teenage mutant ninja turtle like not want ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>Ofcourse. If you can debunk our arguments and ...</td>\n",
              "      <td>ofcourse . debunk argument stuff , , well win ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2500 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Id                                               Text  \\\n",
              "0        0  i love this i am big fun of cross over and Lor...   \n",
              "1        1                                     FUCK LES BRUNS   \n",
              "2        2  Anyone who can pass the test proceeds. The sta...   \n",
              "3        3  yeah you're right. i just glanced over and tho...   \n",
              "4        4                     I get abused on the daily fam    \n",
              "...    ...                                                ...   \n",
              "2495  2495  [NAME] pnr play has not been good, they are de...   \n",
              "2496  2496    You k how it's bad when a furry is roasting you   \n",
              "2497  2497  Don't you know that kids need constant supervi...   \n",
              "2498  2498  DON'T DO THE TEENAGE MUTANT NINJA TURTLES LIKE...   \n",
              "2499  2499  Ofcourse. If you can debunk our arguments and ...   \n",
              "\n",
              "                                           cleaned_text  \n",
              "0                          love big fun cross lore skin  \n",
              "1                                         fuck les brun  \n",
              "2     anyone pass test proceed . standard exist prot...  \n",
              "3     ye relief , happy right . glance think swish l...  \n",
              "4                                   get abuse daily fam  \n",
              "...                                                 ...  \n",
              "2495         pnr play not good , defend jok let get rim  \n",
              "2496                                  k bad furry roast  \n",
              "2497  not know kid need constant supervision turn ? ...  \n",
              "2498  not teenage mutant ninja turtle like not want ...  \n",
              "2499  ofcourse . debunk argument stuff , , well win ...  \n",
              "\n",
              "[2500 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Text</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i love this i am big fun of cross over and Lor...</td>\n",
              "      <td>love big fun cross lore skin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FUCK LES BRUNS</td>\n",
              "      <td>fuck les brun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Anyone who can pass the test proceeds. The sta...</td>\n",
              "      <td>anyone pass test proceed . standard exist prot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>yeah you're right. i just glanced over and tho...</td>\n",
              "      <td>ye relief , happy right . glance think swish l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I get abused on the daily fam</td>\n",
              "      <td>get abuse daily fam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>2495</td>\n",
              "      <td>[NAME] pnr play has not been good, they are de...</td>\n",
              "      <td>pnr play not good , defend jok let get rim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>You k how it's bad when a furry is roasting you</td>\n",
              "      <td>k bad furry roast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>Don't you know that kids need constant supervi...</td>\n",
              "      <td>not know kid need constant supervision turn ? ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>DON'T DO THE TEENAGE MUTANT NINJA TURTLES LIKE...</td>\n",
              "      <td>not teenage mutant ninja turtle like not want ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>Ofcourse. If you can debunk our arguments and ...</td>\n",
              "      <td>ofcourse . debunk argument stuff , , well win ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2500 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Id                                               Text  \\\n",
              "0        0  i love this i am big fun of cross over and Lor...   \n",
              "1        1                                     FUCK LES BRUNS   \n",
              "2        2  Anyone who can pass the test proceeds. The sta...   \n",
              "3        3  yeah you're right. i just glanced over and tho...   \n",
              "4        4                     I get abused on the daily fam    \n",
              "...    ...                                                ...   \n",
              "2495  2495  [NAME] pnr play has not been good, they are de...   \n",
              "2496  2496    You k how it's bad when a furry is roasting you   \n",
              "2497  2497  Don't you know that kids need constant supervi...   \n",
              "2498  2498  DON'T DO THE TEENAGE MUTANT NINJA TURTLES LIKE...   \n",
              "2499  2499  Ofcourse. If you can debunk our arguments and ...   \n",
              "\n",
              "                                           cleaned_text  \n",
              "0                          love big fun cross lore skin  \n",
              "1                                         fuck les brun  \n",
              "2     anyone pass test proceed . standard exist prot...  \n",
              "3     ye relief , happy right . glance think swish l...  \n",
              "4                                   get abuse daily fam  \n",
              "...                                                 ...  \n",
              "2495         pnr play not good , defend jok let get rim  \n",
              "2496                                  k bad furry roast  \n",
              "2497  not know kid need constant supervision turn ? ...  \n",
              "2498  not teenage mutant ninja turtle like not want ...  \n",
              "2499  ofcourse . debunk argument stuff , , well win ...  \n",
              "\n",
              "[2500 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# df_train = pd.read_csv(\"data/cleaned_train.csv\", encoding=\"utf-8\")\n",
        "df_test_problem = pd.read_csv(\"data/cleaned_test.csv\", encoding = \"utf-8\")\n",
        "\n",
        "df_train = pd.read_csv(\"data/train.csv\")\n",
        "df_test = pd.read_csv(\"data/test.csv\")\n",
        "df_train, df_test = common_code.clean_text_wrapper(df_train, df_test)\n",
        "\n",
        "display(df_test)\n",
        "display(df_test_problem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['gratitude', 'disapproval', 'amusement', 'love', 'anger'], dtype='object')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = df_train['Label'].value_counts()\n",
        "result.index"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
